---
title: "Vizburgh Model"
author: "Yuji Shen"
date: "March 3, 2015"
output:
  html_document:
    css: style.css
---

******
Data: [PGHSNAP Raw Data on Google Sheet](https://docs.google.com/spreadsheet/ccc?key=0Ag0xdSSLPcUHdEo0STlkRVBpcVZEcUtwTG9wWjJTd2c#gid=1)

```{r knitr.setting,echo=FALSE}
knitr::opts_chunk$set(warning = FALSE,
                      message = FALSE,
                      echo = F)
```

```{r data}
setwd('/Users/Yuji/Workspace/R/Visburgh/data')
library(ggplot2)
library(gridExtra)
library(MASS)
library(gdata)
library(rgdal)
library(rgeos)
library(caret)
library(car)

data.transform <- function(data){
  for(i in 1:ncol(data)){
    if(sum(grepl("\\$", data[[i]])) > 0){
      data[i] <- gsub("\\$", "", data[[i]])
    }
    if(sum(grepl(",", data[[i]])) > 0){
      data[i] <- as.numeric(gsub(",", "", data[[i]]))
    }
    if(sum(grepl("%", data[[i]])) > 0){
      data[i] <- as.numeric(gsub("%", "", data[[i]]))/100
    }
  }
  data
}

readXLS <- function(file){
  data.list <- list()
  sheetList <- sheetNames(file)
  for(i in 1:length(sheetList)){
    data.list[[sheetList[i]]] <- data.transform(read.xls(file, sheet = sheetList[i], na.strings = "N/A", check.names = FALSE))
  }
  data.list
}

xls.file <- "PGHSNAP.xls"
data.list <- readXLS(xls.file)

# colnames(newdata) <- gsub("\\.+",".",colnames(newdata))

dat1 <- cbind(data.list[[2]][24], data.list[[2]][22], data.list[[5]][9:15])
row.names(dat1) <- as.character(data.list[[1]][[1]])

op <- par()
```

Here are several questions left to answer:

1. When to apply logarithm? Should I use it here?
2. When and how to use glm Poisson model? Why it looks so strange when applied here?
3. Is linear model right here? How to solve normality?
4. If the model is right, how to interpret that some crimes have positive coefficients?

# Housing Sales Price & Crime


## Log Effect on Finacial Data
```{r log transformation,fig.height=3,eval=FALSE}
# cor(dat1, use = "complete.obs")
p1 <- qplot(dat1[[1]], data = dat1, xlim = c(0,400000), xlab = "Median Sale Price", main = "Raw v.s. Log Median Sales Price")
p2 <- qplot(log(dat1[[1]]), data = dat1, xlab = "log(Median Sale Price)")
grid.arrange(p1, p2, ncol = 1)

p3 <- qplot(dat1[[2]], data = dat1, xlab = "Median Home Value", main = "Raw v.s. Log Median Housing Value")
p4 <- qplot(log(dat1[[2]]), data = dat1, xlab = "log(Median Home Value)")
grid.arrange(p3, p4, ncol = 1)
```

After applied logarithm on `Median Sales Price` and `Median Home Value`, right-skewed data looks normal. Although the R-squared in non-logarithmed model is higher, it does not obey the normality well. So I will use logarithmed data in the later steps.

After applied linear regression on whole data, I checked the multicollinearity by VIF. It shows that `#Agr. Assault (2010)` with 5.31 VIF should be removed from the model.

## Linear Regression
```{r linear regression}
fmla <- as.formula(paste("`",colnames(dat1)[1], "`~", paste("`",colnames(dat1[-c(1,6)]),"`", collapse = "+", sep=""), sep=""))

dat1[1:2] <- log(dat1[1:2])

m.l <- lm(fmla, na.omit(dat1[-6]))
# vif(m.l)

par(mar = rep(2, 4))
par(mfrow = c(2, 2))
plot(m.l)
par(op)

summary(m.l)
#train_control <- trainControl(method="repeatedcv", number=10, repeats=10)
#m.linear1 <- train(fmla, data = na.omit(dat1), #trControl=train_control, method="lm")
#getTrainPerf(m.linear1)
```

I add log transformation on `Median Sales Price` and `Median Home Value`. Although the R-squared decreases a little, `Drug` and `Murder` show their effect on the model.

## Linear Regression with PCA on Crime
```{r}
pca.crime <- prcomp(dat1[-c(1,2, 6)], scale = T)
pc.crime <- predict(pca.crime)
pca.crime$rotation
#plot(pca.crime, main = "Screeplot of Crime")

#crime.dist <- dist(dat1[-c(1,2)])
#crime.mds <- cmdscale(crime.dist)
#plot(crime.mds, type = 'n', main = "Multidimensional Scaling on Crime")
#text(crime.mds, labels=data.list[[1]][[1]])

m.lm.pc <- lm(dat1[[1]] ~ dat1[[2]] + pc.crime[,1] + pc.crime[,2])
# par(mar = rep(2, 4))
# par(mfrow = c(2, 2))
# plot(m.lm.pc)
# par(op)
summary(m.lm.pc)
```

In this model, I do PCA on Crime and use PC1 and PC2 in the model. It is interesting that PC1 is not significant but PC2 is. According to the loadings of PC2, which has positive values on `Murder` and `Drug`, this result is consistent with the Linear Regression.

It is common that `Murder` will have an effect on sales price, because it's a severe violence. But **it is surprised to see that `Drug Violations` is fair significant in this model.** Will this still happen when we apply linear regression on U.S. counties data? Let's see!

# What about U.S. Counties Data?

[Uniform Crime Reporting Program Data: County-Level Detailed Arrest and Offense Data, 2012 (ICPSR 35019)](http://www.icpsr.umich.edu/icpsrweb/DSDR/studies/35019)

[Data < Zillow Real Estate Research](http://www.zillow.com/research/data/#zhvi)

I choose all data in 2012, threshold for crime coverage indicator is 90, and only keep counties with more than 6-month data and get their median as price and value in 2012. After merging two datasets, I get 632 counties data records.

```{r county data}
setwd('/Users/Yuji/Workspace/R/Visburgh/data')
crime <- get(load('ICPSR_35019/DS0001/35019-0001-Data.rda'))
crime <- subset(crime, COVIND >= 90)
price <- read.csv('County_MedianSoldPrice_AllHomes.csv', check.names = F)
value <- read.csv('County_Zhvi_AllHomes.csv', check.names = F)
price.2012 <- price[c(1:5, 195:206)]
price.2012 <- price.2012[apply(price.2012, 1, function(x){mean(!is.na(x)) >= 0.5}), ]
value.2012 <- value[c(1:5, 195:206)]
value.2012 <- value.2012[apply(value.2012, 1, function(x){mean(!is.na(x)) >= 0.5}), ]

price.2012$Median.Sale.Price.2012 <- 
  apply(price.2012[-c(1:5)], 1, 
        function(x){
          median(x, na.rm=T)
          })
value.2012$Median.Home.Value.2012 <- 
  apply(value.2012[-c(1:5)], 1, 
        function(x){
          median(x, na.rm=T)
          })

county <- merge(price.2012[-c(6:17)], value.2012[-c(1:3, 6:17)], by = c("StateCodeFIPS", "MunicipalCodeFIPS"))
county <- merge(county, crime[-c(1:4)], 
      by.x = c("StateCodeFIPS", "MunicipalCodeFIPS"), 
      by.y = c("FIPS_ST", "FIPS_CTY"))
county[6:7] <- log(county[6:7])
row.names(county) <- 1:nrow(county)
```

Different with Pittsburgh data, the model with only `Median Home Value` will get a 0.935 R-squared. And I found high multicollinearity between predictors.

## Same Crime with Before
```{r}
m.c <- lm(Median.Sale.Price.2012 ~., county[c(6:7, 16:20, 22, 33)])
vif(m.c)
par(mar = rep(2, 4))
par(mfrow = c(2, 2))
plot(m.c)
par(op)
summary(m.c)
```

We get a very different model with former one. Here, several crimes are significant, but due to multicollinearity, we should not count it as meaningful. And I also found a high correlation between population and many crimes. So I decide to use crime rate as predictor. (I have to remove 2 crimes that are juveniles only)

## Use Crime Rate
```{r}
county.rate <- cbind(county[1:11], county[12:55]/county[[8]]*100)

m.cr <- lm(Median.Sale.Price.2012 ~., data = county.rate[-40, c(6:7,16:20, 22, 33)])
vif(m.cr)
par(mar = rep(2, 4))
par(mfrow = c(2, 2))
plot(m.cr)
par(op)
qqPlot(m.cr, labels=row.names(m.cr), id.method="identify", simulate=TRUE, main="Q-Q Plot")
summary(m.cr)
```

As we can see from VIF, the multicollinearity is solved by using Crime 100 per capita.

## PCA on Crime Rate
```{r}
pca.county <- prcomp(county.rate[-40, c(16:20,22,33)], scale = T)
pc.county <- predict(pca.county)
pca.county$rotation
#plot(pca.county, main = "Screeplot of Crime")

#crime.dist <- dist(dat1[-c(1,2)])
#crime.mds <- cmdscale(crime.dist)
#plot(crime.mds, type = 'n', main = "Multidimensional Scaling on Crime")
#text(crime.mds, labels=data.list[[1]][[1]])


mus.pc <- lm(Median.Sale.Price.2012 ~., data = cbind(county.rate[-40, 6:7], pc.county))
# par(mar = rep(2, 4))
# par(mfrow = c(2, 2))
# plot(m.lm.pc)
# par(op)
summary(mus.pc)
vif(mus.pc)
```

## GLM Poisson
```{r}

glm.cr <- glm(Median.Sale.Price.2012 ~., data = cbind(exp(county[6:7]), county[c(16:20, 22, 33)])[-40,], family = poisson)
summary(glm.cr)
```

I transform price and value back to original and applied poisson glm on count data. It is so strange that all coefficients are significant.

******

#Clustering of Neiborghhoods


I want to explore the relationship between **Planning Sector** and the **overall statistics** (all data in Vizburgh) for neighborhoods. So I do **K-means Cluster** on the entire dataset. Because there are 16 sectors, I set the cluster nubmer to 16. Due to limit of k-means algorithm, I replace the NA with median data.

Here is the sector plot:

```{r plot sector}
dat2 <- cbind(data.list[[1]][c(1, 2, 10, 22:27)],
              data.list[[2]][c(22, 24)],
              data.list[[4]][16],
              data.list[[5]][c(7, 9:15)],
              data.list[[2]][9:11])

row.names(dat2) <- as.character(data.list[[1]][[1]])
#dat2 <- na.omit(dat2)
dat2[-c(1:2)] <- data.frame(apply(dat2[-c(1:2)], 2, function(x){
  x[is.na(x)] <- median(x, na.rm = T)
  x
}))
dat2[-c(1:2)] <- scale(dat2[-c(1:2)])
labels <- as.character(data.list[[1]][[1]])

map <- readOGR("/Users/Yuji/Workspace/Web/iv_prj/pittsburgh.geojson", "OGRGeoJSON", verbose = F, p4s = '+init=epsg:4326', stringsAsFactors=F)

ind <- which(map@data$name == "Observatory Hill")
map@data[ind,1] <- "Perry North"
map@data <- merge(map@data, dat2, by.x = "name", by.y = "Neighborhood", all.x = T, sort = F)

par(mar = c(0,0,2,0))
plot(map, col = rainbow(16)[map@data$Sector], main = "Planning Sector of Pittsburgh")
p <- polygonsLabel(map, map@data$Sector, method = "centroid", cex = 0.8)
par(op)
```

Each of the 16 sectors has unique characteristics shared by the neighborhoods that form that sector. The key components that determine the criteria in the formation of these sectors are:  _Neighborhood Boundaries, Geography, Council Districts, Community Resources, Community Assets, Transportation, School Feeder Patterns, Planning Processes, Neighborhood Character_.

```{r cluster,fig.height=5}


do.mds <- function(dataset, lbls) {
  data.dist = dist(dataset)
  data.mds = cmdscale(data.dist)
  data.mds
}

do.kmeans <- function(dataset, lbls, k=4) {
  set.seed(123)
  data.clu = kmeans(dataset, centers=k, nstart=10)
  data.clu
}
main <- "MDS on Visualizing Dataset"
dat2.mds <- do.mds(dat2[-c(1:2)], labels)
clu <- do.kmeans(dat2[-c(1:2)], labels, k = 16)$cluster
data2 <- data.frame(x=dat2.mds[,1],y=dat2.mds[,2],name=dat2[[1]],sector=factor(dat2[[2]]), clu=factor(clu))

#ggplot(aes(x=x,y=y,color=clu), data=data2) +
#     geom_text(aes(x=x,y=y,color=clu,label=name), size=4)

par(mar = c(0,0,2,0))
plot(dat2.mds, type = "n")
text(dat2.mds, labels, col = rainbow(16)[clu])
title(main = "Cluster of Neighborhoods on MDS")
par(op)

cluster.purity <- function(clusters, classes) {
  sum(apply(table(classes, clusters), 2, max)) / length(clusters)
}
cluster.entropy <- function(clusters,classes) {
  en <- function(x) {
    s = sum(x)
    sum(sapply(x/s, function(p) {if (p) -p*log2(p) else 0} ) )
  }
  M = table(classes, clusters)
  m = apply(M, 2, en)
  c = colSums(M) / sum(M)
  sum(m*c)
}

```

MDS plot shows the cluster members clearly, but is hard to see the corresponding sector numbers. So I make a geo-plot with sector labels.

```{r plot cluster map}
par(mar = c(0,0,2,0))
plot(map, col = rainbow(16)[clu], main = "Cluster Plot with Sector Labels")
p <- polygonsLabel(map, map@data$Sector, method = "centroid", cex = 0.8)
par(op)

eval <- rbind(Purity = cluster.purity(clu, dat2[[2]]), Entropy = cluster.entropy(clu, dat2[[2]]))

knitr::kable(eval, caption = "Clustering Performance")
```

Overall, it seems our clusters do not have a strong relationship with sectors. But the clusters do appear to be spatial adjacent. For example, there are 4 `green` clusters locate in 11 sector, another 2 pairs of them in 4 and 7 sector.

******