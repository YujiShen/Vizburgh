---
title: "Vizburgh Model"
author: "Yuji Shen"
date: "March 3, 2015"
output:
  html_document:
    css: style.css
---

******
* Data: [PGHSNAP Raw Data on Google Sheet](https://docs.google.com/spreadsheet/ccc?key=0Ag0xdSSLPcUHdEo0STlkRVBpcVZEcUtwTG9wWjJTd2c#gid=1)
* [Code on GitHub](https://github.com/YujiShen/Vizburgh)

The purpose of this analysis is trying to find a model to illustrate the relationship between Median Sale Price and several crimes, and hope it will be consistent on different dataset.

Here are several questions left to answer:

1. According to the purpose of INTERPRETING, what should I focus in the model?
1. Is interpreting useless? Should I change my topic and consider predicting or more valuable applications?
1. How to do a complete regression diagnosis and fix the problem? (I did many parts of it, but I still can not pass the global test)
2. Different transformation gives different model, which one should I pick? (Muder and Drug are significant when using number of crime, muder and burglary are significant when using crime rate)
2. Why the county level crime rate are much LOWER than Pittsburgh neighborhood data? (1000 per capita in county-level has the same magnitude with 100 per capita in Pittsburgh neighborhood)
3. With a very low R-squared but significant coefficent, how to interpret the model?
4. How to evaluate the glm with gamma family?

# Housing Sales Price & Crime

Dependent variable: Median Sale Price

Independent variables: Murder, Rape, Robbery, Agr.Assault, Burglary, Auto.Theft, Drug.Violations

Filling missing values in Median Sale Price by `mice` method.

```{r knitr.setting,echo=FALSE}
knitr::opts_chunk$set(warning = FALSE,
                      message = FALSE,
                      echo = FALSE)
```

```{r data,results='hide'}
setwd('/Users/Yuji/Workspace/R/Visburgh/data')
library(ggplot2)
library(MASS)
library(rgdal)
library(rgeos)
library(caret)
library(car)
library(mice)
library(PerformanceAnalytics)
library(gvlma)

load("pitt.RData")
load("county.level.RData")
dat1 <- cbind(data.list[[2]][24], data.list[[5]][9:15])
row.names(dat1) <- as.character(data.list[[1]][[1]])
set.seed(36)
dat1 <- complete(mice(dat1))

names(dat1) <- gsub(" ", ".", names(dat1))
names(dat1) <- gsub("\\.\\.", ".", names(dat1))
names(dat1) <- gsub(".{2}2010.", "", names(dat1))
names(dat1) <- gsub("#", "", names(dat1))

mar <- par()$mar

residplot <- function(fit, nbreaks=10) {
  z <- rstudent(fit)
  hist(z, breaks=nbreaks, freq=FALSE,
        xlab="Studentized Residual",
        main="Distribution of Errors")
  rug(jitter(z), col="brown")
  curve(dnorm(x, mean=mean(z), sd=sd(z)),
        add=TRUE, col="blue", lwd=2)
  lines(density(z)$x, density(z)$y,
        col="red", lwd=2, lty=2)
  legend("topright",
         legend = c( "Normal Curve", "Kernel Density Curve"),
         lty=1:2, col=c("blue","red"), cex=.7)
  }
```

## Linear Regression v.s Log Linear Regression
### Linear Regression
```{r linear regression}
# fmla <- as.formula(paste("`",colnames(dat1)[1], "`~", paste("`",colnames(dat1[-c(1,2)]),"`", collapse = "+", sep=""), sep=""))

m.l <- lm(Median.Sale.Price ~ ., dat1)

cat("### Summary of Linear Model ###")
gvmodel <- gvlma(m.l)
summary(gvmodel)

par(mar = rep(2, 4), mfrow = c(2, 2))
plot(m.l)
par(mar = mar, mfrow = c(1, 1))

#train_control <- trainControl(method="repeatedcv", number=10, repeats=10)
#m.linear1 <- train(fmla, data = na.omit(dat1), #trControl=train_control, method="lm")
#getTrainPerf(m.linear1)
```

### Log Linear Regression
```{r}
dat1.log <- dat1
dat1.log[1] <- log(dat1[1])
m.log <- lm(Median.Sale.Price ~ ., dat1.log)
cat("### Summary of Linear Model with Logarithmed Median Sale Price ###")
gvmodel <- gvlma(m.log)
summary(gvmodel)
par(mar = rep(2, 4), mfrow = c(2, 2))
plot(m.log)
par(mar = mar, mfrow = c(1, 1))
```

The log model has a better R-Squared, and Drug.Violations and Murder are significant. But 3 of 7 crimes with positive coefficients, showing that multicollinearity exists in the model.

## Check Normality
```{r log transformation}
par(mfrow = c(2, 2))
hist(dat1[[1]], breaks = 1000, xlim = c(0, quantile(dat1[[1]], probs = 0.98, na.rm = T)), main = "Histogram of Median Sale Price", xlab = "Median Sale Price")
hist(dat1.log[[1]], breaks = 30, main = "Histogram of Logarithmed Median Sale Price", xlab = "Logarithmed Median Sale Price")
qqPlot(m.l, labels=row.names(m.l), id.method="identify", simulate=TRUE, main="Q-Q Plot")
qqPlot(m.log, labels=row.names(m.l), id.method="identify", simulate=TRUE, main="Q-Q Plot with Logrithmed Data")
par(mfrow = c(1, 1))
```

After applied logarithm on Median Sales Price

* From histogram, right-skewed data looks normal, 
* From Q-Q plot, most points fall close to the line and are within the confidence envelope.

## Solve Multicollinearity
```{r multicollinearity}
chart.Correlation(dat1.log)

cat("### VIF of Log Linear Model ###")
vif(m.log)

m.log2 <- lm(Median.Sale.Price ~ ., dat1.log[-c(4,5,7)])
cat("### Summary of Log Linear Model after Eliminating Robbery, Agr.Assault, Auto.Theft ###")
summary(m.log2)
cat("### VIF of Log Linear Model after Eliminating Robbery, Agr.Assault, Auto.Theft ###")
vif(m.log2)
```

The VIFs of log model are not very large, but result is not good when I tried to eliminate some high VIF variables. So I am thinking about maybe population is an underlying common factor. I transform the number of crime into number of crime per 100 capita, like a larger crime rate.

## Crime Rate insted of Number of Crime
```{r crime rate}
pop <- data.list[[1]][[10]]
dat1.rate <- dat1.log
dat1.rate[-1] <- dat1.rate[-1]/pop*100
m.rate <- lm(Median.Sale.Price ~ ., dat1.rate)
cat("### Summary of Log & Crime Rate Linear Model ###")
gvmodel <- gvlma(m.rate)
summary(gvmodel)
cat("### VIF of Log & Crime Rate Linear Model ###")
vif(m.rate)

m.rate1 <- lm(Median.Sale.Price ~ ., dat1.rate[-c(4,5,7)])
cat("### Summary of Log & Crime Rate Linear Model after Eliminating Robbery, Agr.Assault, Auto.Theft ###")
gvmodel <- gvlma(m.rate1)
summary(gvmodel)
cat("### VIF of Log & Crime Rate Linear Model after Eliminating Robbery, Agr.Assault, Auto.Theft ###")
vif(m.rate1)
```

When using crime rate as independent variables, the R-squared is higher than before. And after eliminating Robbery, Agr.Assault, Auto.Theft, the model implies that the Murder and Burglary are significant, instead of Murder and Drug.Violations. But the model still violates 3 linear regression assumptions.

## Linear Regression with PCA on Crime
```{r}
pca.crime <- prcomp(dat1.rate[-1], scale = T)
pc.crime <- predict(pca.crime)
cat("### Rotation of PCs ###")
pca.crime$rotation
dat1.pca <- cbind(dat1.log[1], pc.crime)

#plot(pca.crime, main = "Screeplot of Crime")

#crime.dist <- dist(dat1[-c(1,2)])
#crime.mds <- cmdscale(crime.dist)
#plot(crime.mds, type = 'n', main = "Multidimensional Scaling on Crime")
#text(crime.mds, labels=data.list[[1]][[1]])

m.pca <- lm(Median.Sale.Price ~ ., dat1.pca)
# par(mar = rep(2, 4))
# par(mfrow = c(2, 2))
# plot(m.lm.pc)
# par(op)
gvmodel <- gvlma(m.pca)
cat("### Summary of PCA Linear Model ###")
summary(gvmodel)
cat("### VIF of PCA Linear Model ###")
vif(m.pca)
```

It is hard to interpret the result of PCA.

What will the model be if we use  U.S. county-level data? Let's see!

***********

# What about U.S. Counties Data?

[Uniform Crime Reporting Program Data: County-Level Detailed Arrest and Offense Data, 2012 (ICPSR 35019)](http://www.icpsr.umich.edu/icpsrweb/DSDR/studies/35019)

[Data < Zillow Real Estate Research](http://www.zillow.com/research/data/#zhvi)

I choose all data in 2012, threshold for crime coverage indicator is 90, and only keep counties with more than 6-month data and get their median as sale price in 2012. After merging two datasets, I get a county-level data with 629 observations.

## Same Crime with Before

Let's do the log on Median Sale Price and get the crime rate for the 7 kinds of crime we have used in Pittsburgh model.

It is very strange that the crime per 100 capita of county level is much LOWER than Pittsburgh data, so I use crime per 1000 capita instead, which is at the same magnitude.

```{r}

dat2 <- county[c(6, 16:20, 22, 33)]
dat2.log.rate <- cbind(log(county[6]), county[c(16:20, 22, 33)]/county[[8]] * 1000)

chart.Correlation(dat2.log.rate)

m.c <- lm(Median.Sale.Price ~., dat2.log.rate)
vif(m.c)
par(mar = rep(2, 4), mfrow = c(2, 2))
plot(m.c)
par(mar = mar, mfrow = c(1, 1))
qqPlot(m.c, labels=row.names(m.c), id.method="identify", simulate=TRUE, main="Q-Q Plot")
residplot(m.c)

gvmodel <- gvlma(m.c)
cat("### Summary of Linear Model ###")
summary(gvmodel)
```

## Eliminating the Variables & Compared to Pittsburgh
```{r}
m.c1 <- lm(Median.Sale.Price ~., dat2.log.rate[-c(4,5,7)])
cat("### Summary of Pittsburgh Linear Model ###")
summary(m.rate1)
cat("### Summary of County Level Linear Model ###")
summary(m.c1)
```

Something different with Pittsburgh model

* R-squared is much lower
* Murder is not strong significant, and the coefficient is much small
* Drug is still positive but became significant

## PCA on Crime Rate
```{r,eval=FALSE}
pca.county <- prcomp(county.rate[-40, c(16:20,22,33)], scale = T)
pc.county <- predict(pca.county)
pca.county$rotation
#plot(pca.county, main = "Screeplot of Crime")

#crime.dist <- dist(dat1[-c(1,2)])
#crime.mds <- cmdscale(crime.dist)
#plot(crime.mds, type = 'n', main = "Multidimensional Scaling on Crime")
#text(crime.mds, labels=data.list[[1]][[1]])


mus.pc <- lm(Median.Sale.Price.2012 ~., data = cbind(county.rate[-40, 6:7], pc.county))
# par(mar = rep(2, 4))
# par(mfrow = c(2, 2))
# plot(m.lm.pc)
# par(op)
summary(mus.pc)
vif(mus.pc)
```

## GLM Gamma
```{r}
m.gamma <- glm(Median.Sale.Price ~., data = dat2.log.rate, family = "Gamma")
cat("### Summary of Gamma Model ###")
summary(m.gamma)
cat("### VIF of Gamma Model ###")
vif(m.gamma)
```


******

#Clustering of Neiborghhoods


I want to explore the relationship between **Planning Sector** and the **overall statistics** (all data in Vizburgh) for neighborhoods. So I do **K-means Cluster** on the entire dataset. Because there are 16 sectors, I set the cluster nubmer to 16. Due to limit of k-means algorithm, I replace the NA with median data.

Here is the sector plot:

```{r plot sector}
dat3 <- cbind(data.list[[1]][c(1, 2, 10, 22:27)],
              data.list[[2]][c(22, 24)],
              data.list[[4]][16],
              data.list[[5]][c(7, 9:15)],
              data.list[[2]][9:11])

row.names(dat3) <- as.character(data.list[[1]][[1]])
#dat3 <- na.omit(dat3)
dat3[-c(1:2)] <- data.frame(apply(dat3[-c(1:2)], 2, function(x){
  x[is.na(x)] <- median(x, na.rm = T)
  x
}))
dat3[-c(1:2)] <- scale(dat3[-c(1:2)])
labels <- as.character(data.list[[1]][[1]])

map <- readOGR("/Users/Yuji/Workspace/Web/iv_prj/pittsburgh.geojson", "OGRGeoJSON", verbose = F, p4s = '+init=epsg:4326', stringsAsFactors=F)

ind <- which(map@data$name == "Observatory Hill")
map@data[ind,1] <- "Perry North"
map@data <- merge(map@data, dat3, by.x = "name", by.y = "Neighborhood", all.x = T, sort = F)

par(mar = c(0,0,2,0))
plot(map, col = rainbow(16)[map@data$Sector], main = "Planning Sector of Pittsburgh")
p <- polygonsLabel(map, map@data$Sector, method = "centroid", cex = 0.8)
par(mar = mar)
```

Each of the 16 sectors has unique characteristics shared by the neighborhoods that form that sector. The key components that determine the criteria in the formation of these sectors are:  _Neighborhood Boundaries, Geography, Council Districts, Community Resources, Community Assets, Transportation, School Feeder Patterns, Planning Processes, Neighborhood Character_.

```{r cluster,fig.height=5}


do.mds <- function(dataset, lbls) {
  data.dist = dist(dataset)
  data.mds = cmdscale(data.dist)
  data.mds
}

do.kmeans <- function(dataset, lbls, k=4) {
  set.seed(123)
  data.clu = kmeans(dataset, centers=k, nstart=10)
  data.clu
}
main <- "MDS on Visualizing Dataset"
dat3.mds <- do.mds(dat3[-c(1:2)], labels)
clu <- do.kmeans(dat3[-c(1:2)], labels, k = 16)$cluster
data2 <- data.frame(x=dat3.mds[,1],y=dat3.mds[,2],name=dat3[[1]],sector=factor(dat3[[2]]), clu=factor(clu))

#ggplot(aes(x=x,y=y,color=clu), data=data2) +
#     geom_text(aes(x=x,y=y,color=clu,label=name), size=4)

par(mar = c(0,0,2,0))
plot(dat3.mds, type = "n")
text(dat3.mds, labels, col = rainbow(16)[clu])
title(main = "Cluster of Neighborhoods on MDS")
par(mar = mar)

cluster.purity <- function(clusters, classes) {
  sum(apply(table(classes, clusters), 2, max)) / length(clusters)
}
cluster.entropy <- function(clusters,classes) {
  en <- function(x) {
    s = sum(x)
    sum(sapply(x/s, function(p) {if (p) -p*log2(p) else 0} ) )
  }
  M = table(classes, clusters)
  m = apply(M, 2, en)
  c = colSums(M) / sum(M)
  sum(m*c)
}

```

MDS plot shows the cluster members clearly, but is hard to see the corresponding sector numbers. So I make a geo-plot with sector labels.

```{r plot cluster map}
par(mar = c(0,0,2,0))
plot(map, col = rainbow(16)[clu], main = "Cluster Plot with Sector Labels")
p <- polygonsLabel(map, map@data$Sector, method = "centroid", cex = 0.8)
par(mar = mar)

eval <- rbind(Purity = cluster.purity(clu, dat3[[2]]), Entropy = cluster.entropy(clu, dat3[[2]]))

knitr::kable(eval, caption = "Clustering Performance")
```

Overall, it seems our clusters do not have a strong relationship with sectors. But the clusters do appear to be spatial adjacent. For example, there are 4 `green` clusters locate in 11 sector, another 2 pairs of them in 4 and 7 sector.

******